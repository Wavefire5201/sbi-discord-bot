import discord
from discord.commands import option
from discord.ext import commands
from google import genai
from utils import get_logger
from utils.config import GEMINI_API_KEY, LLM_MODEL

logger = get_logger(__name__)
client = genai.Client(api_key=GEMINI_API_KEY)


class AI(commands.Cog):
    def __init__(self, bot):
        self.bot = bot

    async def send_long_message(self, channel, content: str, max_length: int = 2000):
        """Send a long message by splitting it into chunks if necessary."""
        content_str = str(content)
        if len(content_str) <= max_length:
            await channel.send(content_str)
        else:
            chunks = []
            current_chunk = ""

            for line in content_str.split("\n"):
                # If adding this line would exceed the limit, save current chunk and start new one
                if current_chunk and len(current_chunk) + len(line) + 1 > max_length:
                    chunks.append(current_chunk)
                    current_chunk = line
                else:
                    # Add line to current chunk
                    if current_chunk:
                        current_chunk += "\n" + line
                    else:
                        current_chunk = line

            # Add the last chunk if it has content
            if current_chunk:
                chunks.append(current_chunk)

            # Send all chunks
            for chunk in chunks:
                await channel.send(chunk)

    async def create_gemini_history(self, message_history):
        gemini_history = []

        for msg in reversed(message_history[:-1]):  # Exclude the current message
            if msg.author == self.bot.user:
                gemini_history.append(
                    {"role": "model", "parts": [{"text": msg.content}]}
                )
            else:
                gemini_history.append(
                    {"role": "user", "parts": [{"text": msg.content}]}
                )

        return gemini_history

    ai = discord.SlashCommandGroup("ai", "AI commands")

    @ai.command(name="joke", description="Get a random joke generated by AI!")
    async def joke(self, ctx: discord.ApplicationContext):
        try:
            await ctx.defer()
            response = await client.aio.models.generate_content(
                model="gemini-2.5-flash-lite-preview-06-17",
                contents="Tell me a random funny joke.",
            )
            await ctx.followup.send(str(response.text))
        except Exception as e:
            logger.error(f"Error in joke command: {e}")
            await ctx.followup.send(
                "Sorry, I couldn't generate a joke right now. Please try again later!"
            )

    @ai.command(name="ask", description="Ask AI a question.")
    @option(name="question", description="Question to ask AI")
    async def ask(self, ctx: discord.ApplicationContext, question: str):
        try:
            await ctx.defer()
            response = await client.aio.models.generate_content(
                model="gemini-2.5-flash-lite-preview-06-17",
                contents=f"The user is asking you a question. Please limit your response to less than 500 words. Question: {question}",
            )
            await ctx.followup.send(response.text)
        except Exception as e:
            logger.error(f"Error in chat command: {e}")
            await ctx.followup.send(
                "Sorry, I couldn't generate a response right now. Please try again later!",
                ephemeral=True,
            )

    @ai.command(name="chat", description="Start a new chat with AI.")
    @option(name="prompt", description="Prompt to start the chat with.")
    async def chat(self, ctx: discord.ApplicationContext, prompt: str):
        try:
            await ctx.respond("Starting a new chat!", ephemeral=True)
            title = await client.aio.models.generate_content(
                model=LLM_MODEL,
                contents=f"Based on this prompt, create a simple title for a chat title. Only return the chat title. Question: {prompt}",
            )
            thread: discord.Thread = await ctx.channel.create_thread(
                name=title.text, type=discord.ChannelType.private_thread
            )
            await thread.add_user(ctx.author)

            initial = await thread.send("Thinking...")
            response = await client.aio.models.generate_content(
                model=LLM_MODEL,
                contents=f"{prompt}",
            )
            await initial.delete()

            await self.send_long_message(thread, response.text)

        except Exception as e:
            logger.error(f"Error in chat command: {e}")
            await ctx.followup.send(
                "Sorry, I couldn't start a new chat right now. Please try again later!",
                ephemeral=True,
            )

    @ai.command(
        name="tokens",
        description="Returns the token count of the current conversation.",
    )
    async def tokens(self, ctx: discord.ApplicationContext):
        if not isinstance(ctx.channel, discord.Thread):
            await ctx.respond("This isn't a chat thread!", ephemeral=True)
            return

        t = await ctx.channel.history().flatten()
        gemini_history = [t.content for t in t]
        response = await client.aio.models.count_tokens(
            model=LLM_MODEL,
            contents="\n".join(gemini_history),
        )

        await ctx.respond(
            f"This chat contains {response.total_tokens} tokens.", ephemeral=True
        )

    @commands.Cog.listener()
    async def on_message(self, message: discord.Message):
        # TODO: add all chat thread ids to db, for better identification
        if not isinstance(message.channel, discord.Thread):
            return

        if message.author == self.bot.user:
            return

        # TODO: add a cache in the future to speed up operations
        thread = message.channel
        gemini_history = await self.create_gemini_history(
            await thread.history().flatten()
        )
        # logger.info(f"Gemini history: {gemini_history}")

        chat = client.aio.chats.create(model=LLM_MODEL, history=gemini_history)
        response = await chat.send_message(message.content)
        await self.send_long_message(thread, response.text)


def setup(bot):
    bot.add_cog(AI(bot))
